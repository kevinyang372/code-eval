# Plagiarism Detection Explained

Due to the complex nature of source code plagiarism detection problem, many of the algorithms attempting to address it tend to focus on a specific set of aspects as the evidence for categorization. Thus, it is challenging to fully cover all cases of plagiarism using one single ominous algorithm. To provide as much trustability and insight into the plagiarism detection problem as possible, the Autograder uses a fleet of different approaches and present the result as a summarization of the output of each individual approach. This how-to article aims to 1) explain how each algorithm works behind the scene 2) show the strength and weakness of the algorithms using some attack case experiments.

## Algorithms Used
The Autograder's plagiarism detection module consists of six different algorithms:
* Exact Match
* Unifying AST Match
* Unifying AST Match (ignore variable names)
* Tree Edit Distance (RTED)
* Winnowing
* Comment Edit Distance (Levenshtein)

Most of the algorithms mentioned above took use of the Abstract Syntax Tree (AST), which refers to the tree representation for the abstract syntactic structure of source code. It is worth noticing that AST only covers the structural and content-wise detail of the code and ignores aspects that are irrelevant to the execution outcome of it (e.g. comments and grouping parentheses), which is beneficial for source code plagiarism detection.

### Exact Match
As its name suggests, the exact match algorithm compares each node within an AST and only returns true if the relationship between all the nodes and their contents are identical. A positive output of the exact match algorithm strongly suggests that the two pieces code are almost exactly the same despite the possibility of small changes (e.g. comments / whitespaces).

### Unifying AST Match
The unifying AST match is a match between the abstract syntax trees of two programs that finds a one-to-one mapping between the variable names of the two programs. This category can then allow programs in which all variables have been renamed in a naive fashion.

The 'ignore variable names' variant is more general of the naive Unifying AST Match approach as it completes ignores the id/name of the specific variables. Therefore, it can detect reordering variables using the commutative property of mathematical equations (e.g. a * b = b * a).

### Tree Edit Distance
The Tree Edit Distance algorithm compares two AST on a higher level by calculating the smallest number of node-edit operations necessary to transform one tree to another. The higher the tree edit distance, the lower the chance of plagiarism. The specific implementation in Autograder utilizes the RTED algorithm, whose detail could be found in the paper _RTED: A Robust Algorithm for the Tree Edit Distance_.

### Winnowing
The winnowing algorithm does not rely on AST to compare the similarity between two code files. Instead, it does so by comparing the 'fingerprint' generated by sampling the code content. Theoretically, one could analyze two documents by tokenizing each piece of content, group them into chunks of different sizes, and compare how many times a chunk from one document appears in another. However, this approach is highly time-consuming as it needs to scan through chunks of all different sizes. The winnowing algorithm provides an efficiently way of chunk comparison by utilizing fingerprints and is the core behind the Stanford Moss software. More details could be found in the paper _Winnowing: Local Algorithms for Document Fingerprinting_.

### Comment Edit Distance
The algorithms above all prunes away comments as a source for classification. However, sometimes students would simply copy and paste code contents, making comment a valuable evidence as well. The comment edit distance approach utilizes the Levenshtein string edit distance algorithm to calculate the similarity between the comments of two documents.

## Attack Cases
The following section introduces the sample attack cases used to validate the performance of different source code plagiarism detection algorithms mentioned above. The cases include:
* Change Comments
* Rename Variables
* Change Variable Order
* Change Function Order
* Totally Different File (Control Case)

The original document to compare against looks like:
```
def func(c, d):
    # this is just a comment
    return c + d


def func2(c, d):
    """function that multiplies two arguments together

    param::c => integer
    param::d => integer
    return type => integer
    """
    return c * d
```

### - Change Comments
```
def func(c, d):
    # this is just a comment
    # this is another comment
    return c + d


def func2(c, d):
    """function that multiplies some arguments

    param::c => integer
    param::d => integer
    """
    return c * d
```
This case simply edit some comments and doc-strings in the file and leaves the remaining contents untouched. The performance results are:
```
For Change Comment
======================
* Exact Match result:
True

* Unifying AST result:
True

* Unifying AST (Ignore Variables) result:
True

* Tree Edit Distance result:
1.0

* Winnowing result:
Original Overlap Percentage: 1.0
Test File Overlap Percentage: 1.0

* Comment Edit Distance result:
0.5486725663716814

```
As we expected, all the algorithms successfully identified that the two documents are exactly the same. The comment edit distance doesn't suggest a significant overlap between the comment section as around 50% of one document needs to be changed to match the another. It is worth noticing that since the Exact Match algorithm is based on AST, it automatically prunes away the comments and doc-strings.

### - Rename Variables
```
def func(a, b):
    # this is just a comment
    return a + b


def func2(a, b):
    """function that multiplies two arguments together

    param::x => integer
    param::y => integer
    return type => integer
    """
    return a * b

```
This case focuses on renaming the variable names in a way that won't fundamentally change the functioning of the original source code. The performance results are:
```
For Rename Variable
======================
* Exact Match result:
False

* Unifying AST result:
True

* Unifying AST (Ignore Variables) result:
True

* Tree Edit Distance result:
0.84

* Winnowing result:
Original Overlap Percentage: 1.0
Test File Overlap Percentage: 1.0

* Comment Edit Distance result:
0.9823008849557522

```
The Exact Match algorithm is not able to capture the plagiarism in this case as the contents are no longer exactly the same. Also the tree edit distance also shows difference as some renaming is required to transform one tree to another.

### - Change Variable Order
```
def func(c, d):
    # this is just a comment
    return d + c


def func2(c, d):
    """function that multiplies two arguments together

    param::c => integer
    param::d => integer
    return type => integer
    """
    return d * c

```
This case changes the order of variables using the commutative property of mathematical equations. The performance results are:
```
For Change Variable Order
======================
* Exact Match result:
False

* Unifying AST result:
False

* Unifying AST (Ignore Variables) result:
True

* Tree Edit Distance result:
0.84

* Winnowing result:
Original Overlap Percentage: 1.0
Test File Overlap Percentage: 1.0

* Comment Edit Distance result:
1.0
```
The Unifying AST match failed because the variables in both files do not have a one-to-one mapping with each other ('c' was mapped to 'c', but then later mapped to 'd'). Other than that, all the remaining algorithms gave pretty accurate result.

### - Change Function Order
```
def func2(c, d):
    """function that multiplies two arguments together

    param::c => integer
    param::d => integer
    return type => integer
    """
    return c * d


def func(c, d):
    # this is just a comment
    return c + d

```
This case swaps the ordering of two functions `func` and `func2`. The performance results are:
```
For Change Function Order
======================
* Exact Match result:
False

* Unifying AST result:
False

* Unifying AST (Ignore Variables) result:
False

* Tree Edit Distance result:
0.6799999999999999

* Winnowing result:
Original Overlap Percentage: 0.8461538461538461
Test File Overlap Percentage: 0.8461538461538461

* Comment Edit Distance result:
0.6814159292035398

```
Changing function order successfully deceives many algorithms that scan the files' AST in order as they do not maintain a mapping between every element in both files. The tree edit distance based algorithm also took a dip as now a significant portion of the tree needs to be swapped for matching. The winnowing algorithm remains to give a relatively high confidence for plagiarism as it is based on fingerprinting, which is less sensitive to the ordering of elements.

### - Different Files (Control Case)
```
def func(c, d):
    # returns sum of c and d
    return sum(c, d)


def func2(c, d):
    # returns the multiplication of c and d
    multi = c * d
    return multi

```
While maximizing accuracy for identifying plagiarism, we don't want to drastically increase false positives as well since they will create a file base with significant noise. The performance results are:
```
For Different
======================
* Exact Match result:
False

* Unifying AST result:
False

* Unifying AST (Ignore Variables) result:
False

* Tree Edit Distance result:
0.6206896551724138

* Winnowing result:
Original Overlap Percentage: 0.8461538461538461
Test File Overlap Percentage: 0.3333333333333333

* Comment Edit Distance result:
0.18584070796460173

```
Since the test file is relatively short, there could still remain some noise in the plagiarism detection as some algorithms gave comparatively positive results. It is worth noticing that winnowing gave two very different percentage for original overlap and test file overlap, which is suggesting some fingerprints in original file (e.g. c, d, and c * d) still appears in the test file but not in the reverse (e.g. sum never appears in the original file). One method to address the accuracy of winnowing is to change the k-gram value. The higher the k-gram value, the less the false positives with a cost of higher false negatives.
