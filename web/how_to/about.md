# About Autograder
## Motivation

Currently, the grading process for assignments and pre-class works in Minerva’s computer science department resembles a workflow similar to the following:<br>
 1. Professor publishes the description of the problem statements.<br>
 2. Students submit their answers before the deadline.<br>
 3. After all assignments have been submitted or the deadline has passed, professors could start the grading process.<br>
 4. Students will receive feedback once their professors completed the grading process.<br>

While the grading process in use ensures fairness for students and is standardizable across different class sessions, it also introduces some disadvantages. First of all, the frequency of feedback is low -- for most of the students, it is limited to once per assignment that occurs after the professors release the grades. The frequency of feedback is closely tied to the effectiveness of learning. Sometimes, mistakes like syntax errors and misunderstandings of the problem statement could impede more in-depth feedback on the material despite being easy to discover. If professors are able to provide early feedback, it could prevent such problems from happening. Furthermore, manually grading all the assignments could be a time-consuming process since professors need to repetitively run the submitted answers through the test cases and debug the potential errors. The various configurations and environments students use in their submission could further complicate the grading process.

An automated code evaluation system could effectively solve both problems by allowing students to run their code against a set of defined test cases and receive automatic feedback on aspects like correctness and styling. On top of that, it could be the platform for additional functionalities like plagiarism detection, which helps professors better analyze a large number of student submissions.

## Key Components

When developing the application, I have encountered some major challenges, which require careful consideration of different design choices and further innovation upon existing solutions to ensure the solution will work well in the use case it intends to target. This section will introduce two of them that significantly influence the application: sandboxing testing environment and plagiarism detection algorithms.

#### Sandboxing Testing Environment
While testing code in a local environment is relatively straightforward, encompassing a testing framework within a web application comes with difficult challenges in keeping each testing run independent and parallelizable. For example, a submission that raises an error when running should not interfere with the process of evaluating other submissions. Furthermore, Autograder should also limit user-created code access from dangerous operations such as deleting files, modifying databases, and showing records without the proper permissions.

However, using only the built-in functions provided by Python (e.g., eval or exec) is not sufficient to tackle the problem as the compiler assumes the same accessibility level across main and sub spawned processes. Some other plausible solutions include executing the untrusted code in a different environment, such as virtual machines and containers. Similarly, these options failed to fully solve the problem as creating, maintaining, and despawning a completely isolated execution environment is hugely resource and time costly.

Therefore, the approach Autograder takes in solving the problem comes with two steps. First, all the user submitted code needs to be re-compiled to remove all classes and methods that are unrelated to answer evaluation and could threaten the security of the overall system. `RestrictedPython` is a third-party package that defines a subset of the Python language allowing an untrusted program input to be executed in a trusted environment. The package does so by replacing flagged methods with alternative names, which will cause run-time errors if they are not further defined in the global variables hashmap in the execution stage. For example, 'print' will be re-compiled into '_print_,' a variable not currently declared in Python default.

Re-compiling alone is not a sufficient solution as some particular scenarios like infinite loops and deadlock could still interfere with the normal functioning of Autograder as they will gradually deplete the available resources. Thus, the second step of the solution works by spawning a separate process for each running submission and killing it after a finite time. Although using threads could seem to be a better working alternative as it also provides concurrency in the execution without additional resources for setup and teardown, it could degenerate into the naive solution in case of multiple infinite loops in submission as spawned threads could still take computing resources even after being detached. On the contrary, because different processes use different storage and computation units, the killed processes could be easily cleaned out of memory without much overhead.

#### Plagiarism Detection Algorithms
Plagiarism detection fits perfectly with the use case of the Autograder, as manually comparing all submissions pair by pair is almost impossible in a large volume of answers. Theoretically, it has been proven that calculating the exact similarity between two documents belongs to NP by transforming the computation into a graph isomorphism problem. Consider each component in a document (variables in the case of code) as nodes and the connection between components as edges, the similarity between documents could be defined as whether there exists a bijection f, which could map every edge in one document to one in another (Lukácsy, 2005). Because of NP problems’ nature, the computational cost of calculating the exact similarity level will grow exponentially with the length of documents, which is impossible for Autograder to implement. Despite the challenges, there have been many proposals for methods approximating the possibility of plagiarism within polynomial time.

However, different from plagiarism in essays or other forms of work, plagiarism in code could be hard to catch due to various mutations that could be applied to a block of code without changing its meanings. Some common ones include *replacing variable names, swapping line orders, adding comments, etc*. The unique challenges in source code plagiarism invalidate many commonly used algorithms like exact phrase matching and normalization (Mondal, 2018) since the connection and interaction between the variables are more important than the name of the variables themselves.

A couple of third party organizations offer artificial intelligence backed solutions for source code plagiarism detection, with the most prominent ones like Codeleaks and Moss by Stanford (Aiken, 2004). The main pain points of using a third-party package are the difficulty of integration and customizability. For example, Moss requires its users to post the files for comparison to its endpoint and returns a web page link for all the comparison results. The additional API calling could significantly reduce the available bandwidth for service due to the traffic of posting files and further lead to time costs that are largely uncertain waiting for the response. On top of that, it will also pose challenges on deriving statistics for questions like ‘what are the top 5% of plagiarism results’ as all the analysis is returned in the form of web pages. In the aspect of customizability, machine learning models tend to return a singular result as the output. The lack of supporting evidence and derivation process reduces the credibility of trusting the analysis results. It also means that breaking down and customizing the plagiarism detection model depending on user needs is hardly possible with the shortage of information. Therefore, implementing and innovating on top of existing algorithms could be a better long term solution after summarizing the above tradeoffs,

Autograder applies an effective solution to the problem by analyzing the AST (Neamtiu, 2005) of a piece of code rather than its plain text. An `Abstract Syntax Tree (AST)` is a tree representation of the abstract syntactic structure of source code written in a programming language. Each node of the tree denotes a construct occurring in the source code. AST trees capture not only the written representation but also the relationships between each variable and the specific operators that are applied to them. By parsing the code into an AST tree, Autograder can capture its crucial structure while filtering out other unrelated information.

The question that naturally arises in succession is what information should be included in the plagiarism detection process and what should be ignored. However, the tradeoff could be a double-edged sword -- while including less information could help identify plagiarism even with all the various mutations applied, it also increases the false positive rate as some entirely different code that looks structurally similar might get classified as similar (Avery, 2015). To avoid the dilemma, Autograder uses a spectrum of plagiarism detection algorithms from the `exact match`, which includes all information of an AST tree to `Zhang Shasha's tree edit distance` (Zhang, 1992), which focuses on measuring the degree to which one tree will need to mutate to match another to compile the plagiarism report.
